{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/momo54/large_scale_data_management/blob/main/Spark_Structured_Streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "SzXrtqLokjLR"
      },
      "source": [
        "#  Spark Structured Streaming\n",
        "## Introduction\n",
        "\n",
        "In this lab, we want to understand basics of SPARK STRUCTURED STREAMING.\n",
        "\n",
        "Just consider an infinite stream of events: \n",
        "* the stream of wikipedia updates, \n",
        "* the stream of twitter messages with an hashtag, \n",
        "* the stream of measurement from a sensor network,\n",
        "* The stream can be also a combination of streams coming from many sources\n",
        "\n",
        "Streams raises issues on Volumes and Velocity: possible to have very high volume very quickly...\n",
        "\n",
        "You want to compute virtually anything on the stream:\n",
        "* you want to translate wikipedia update from a language to another\n",
        "* You want to compute the top10 of people contributing to wikipedia\n",
        "* You want train a model to detect vandalism on Wikipedia\n",
        "* You want to analyze tweets on your company to detect positive/Negative sentiments\n",
        "* You want to detect anomaly on your sensor network to raise an alert\n",
        "* use-case  are endless...\n",
        "\n",
        "The challenge is to allow such computation with high volume and high velocity.\n",
        "\n",
        "Main actors in Data Stream Processing are:\n",
        "* Kafka, https://fr.wikipedia.org/wiki/Apache_Kafka \n",
        "* Storm, https://fr.wikipedia.org/wiki/Apache_Storm\n",
        "* Flume, https://flume.apache.org/\n",
        "* Flink, https://fr.wikipedia.org/wiki/Apache_Flink \n",
        "* Kinesis, \n",
        "\n",
        "SPark propose 2 models of STREAMING, DStream and Structured Streams. Structured is now the maintained model. Structured Streaming allows Exactly one semantics ie. one event is processed *exactly once*.\n",
        "\n",
        "This is very important to handle fault tolerance as stream processing is distributed and many fault may happen. One key challenge for stream processing is to process all events exactly once, even if some part of processing have to be restarted due to a failure.\n",
        "\n",
        "The key idea in Structured Streaming is to treat a live data stream as a table that is being continuously appended (even if the table is not fully materialized, processed events are discarded when consumed and replaced by partial aggregates for exmample)\n",
        "\n",
        "![stream model](https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png)\n",
        "\n",
        "The Figure bloew illustrates the Wordcount example. You receive words incrementally:\n",
        "* Each time words is consumed, it is appended to the input table\n",
        "* This triggers the activation of wordcount program that count how many times a word is seen (notice that, once aggregation is done, we can discard inputs in the input table)\n",
        "* Next, results are printed to the output....\n",
        "\n",
        "It is possible:\n",
        "* to print all the the results at each activation. It is the \"complete\" output mode -> make senses for aggregation queries (group by/Count)\n",
        "* to print only new elements appended in the *result* table -> nice for select queries (select(*) where (age>12))\n",
        "* to print only elements that have been updated in the *result* table\n",
        "\n",
        "\n",
        "![stream model](https://spark.apache.org/docs/latest/img/structured-streaming-example-model.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "Lets install code and data.\n",
        "Please note that we will not use clustering -> only one master\n",
        "We just want to see the programming model, not the performances..."
      ],
      "metadata": {
        "id": "OCKP4Tb5rraC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install software\n",
        "# No clustering for this install !!\n",
        "!pip install pyspark\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znIhQWn5nqx4",
        "outputId": "77fff5ad-5152-445d-9093-e6942de924fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 50 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 27.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=30d66e76230f17baff2e126f5a0e9f02ab0ead6cb1d101cc00772e714b779ab2\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get data\n",
        "!wget -O data.txt \"https://raw.githubusercontent.com/apache/spark/master/docs/graphx-programming-guide.md\" \n",
        "\n",
        "# split the data in small files to simulate bag of words arrival\n",
        "! split -l 100 --additional-suffix=\".txt\" data.txt data-\n",
        "! ls -l\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8QPxo_loAbZ",
        "outputId": "3631a12b-b62a-4fec-f45b-6026f650d868"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-19 19:16:16--  https://raw.githubusercontent.com/apache/spark/master/docs/graphx-programming-guide.md\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53517 (52K) [text/plain]\n",
            "Saving to: ‘data.txt’\n",
            "\n",
            "\rdata.txt              0%[                    ]       0  --.-KB/s               \rdata.txt            100%[===================>]  52.26K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-10-19 19:16:16 (4.64 MB/s) - ‘data.txt’ saved [53517/53517]\n",
            "\n",
            "total 140\n",
            "-rw-r--r-- 1 root root  6378 Oct 19 19:16 data-aa.txt\n",
            "-rw-r--r-- 1 root root  5156 Oct 19 19:16 data-ab.txt\n",
            "-rw-r--r-- 1 root root  4682 Oct 19 19:16 data-ac.txt\n",
            "-rw-r--r-- 1 root root  4940 Oct 19 19:16 data-ad.txt\n",
            "-rw-r--r-- 1 root root  5164 Oct 19 19:16 data-ae.txt\n",
            "-rw-r--r-- 1 root root  5160 Oct 19 19:16 data-af.txt\n",
            "-rw-r--r-- 1 root root  5081 Oct 19 19:16 data-ag.txt\n",
            "-rw-r--r-- 1 root root  5029 Oct 19 19:16 data-ah.txt\n",
            "-rw-r--r-- 1 root root  6234 Oct 19 19:16 data-ai.txt\n",
            "-rw-r--r-- 1 root root  5693 Oct 19 19:16 data-aj.txt\n",
            "-rw-r--r-- 1 root root 53517 Oct 19 19:16 data.txt\n",
            "drwxr-xr-x 1 root root  4096 Oct 18 13:36 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## programming\n",
        "\n",
        "Lets starts writing programs..."
      ],
      "metadata": {
        "id": "Z51AO_qCr7Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start the spark session\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredNetworkWordCount\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "dNJa1heFn3LO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We consider the directory streaming as a source for streaming ie. each time a new file appears in this directory -> trigger word count !!\n"
      ],
      "metadata": {
        "id": "A5wWbVASsZn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an empty streaming directory\n",
        "# consider the directory \"streaming\" as a source of streaming\n",
        "# currently it is empty...\n",
        "!mkdir streaming"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGPpskkev8tX",
        "outputId": "ab58d715-fd2b-494b-cdf6-3215ead5edf6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘streaming’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare the directory \"Streaming\" as a source\n",
        "lines = spark.readStream.text(\"streaming\")\n",
        "\n",
        "# and write the \"wordcount program\"\n",
        "words = lines.select(\n",
        "   explode(\n",
        "       split(lines.value, \" \")\n",
        "   ).alias(\"word\")\n",
        ")\n",
        "\n",
        "# Generate running word count\n",
        "wordCounts = words.groupBy(\"word\").count()\n"
      ],
      "metadata": {
        "id": "_ptEvyutsCy4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stopping all active queries\n",
        "# you will understand ;-)\n",
        "for q in spark.streams.active:\n",
        "  print(\"stopping\",q.name)\n",
        "  q.stop()\n",
        "spark.sql(f\"drop table wordcount\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_nZc1jj2niK",
        "outputId": "6cb58916-2ef9-4f1c-899b-859e86b90ffa"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stopping wordcount\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code works... but not in the jupyter console...\n",
        "#query = wordCounts \\\n",
        "#    .writeStream \\\n",
        "#    .outputMode(\"complete\") \\\n",
        "#    .format(\"console\") \\\n",
        "#    .start()\n",
        "\n",
        "\n",
        "# So we write ouput in the \"results\" directory\n",
        "\n",
        "query = wordCounts \\\n",
        "    .writeStream \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .queryName(\"wordcount\") \\\n",
        "    .start()\n",
        "\n",
        "\n",
        "#from time import sleep\n",
        "#for x in range(3):\n",
        "#    spark.sql(\"select * from wordcount order by count DESC\").show(3)\n",
        "#    spark.sql(\"select count(*) from wordcount\").show()  \n",
        "#    sleep(2)\n",
        "\n",
        "\n",
        "# Once running drop one by one data-??.txt files in the streaming directory\n",
        "# and check the result directory\n",
        "# stop execution when happy...\n",
        "#query.awaitTermination()\n",
        "\n",
        "print(query.isActive)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzsKZ1QDuS--",
        "outputId": "bcdeb65f-c382-498c-93fd-fd15954d631e"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from wordcount order by count DESC\").show(3)\n",
        "spark.sql(\"select count(*) from wordcount\").show()\n",
        "!cp data-aa.txt streaming\n",
        "# and wait a little...\n",
        "!sleep 5s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns9mASID7khC",
        "outputId": "8e192d8b-fcb2-40d9-a386-8c65fb1e0243"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "+----+-----+\n",
            "\n",
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|       0|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from wordcount order by count DESC\").show(3)\n",
        "spark.sql(\"select count(*) from wordcount\").show()\n",
        "!cp data-ab.txt streaming\n",
        "# and wait a little\n",
        "!sleep 5s\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67dgCbh675gy",
        "outputId": "278387b4-3ab6-4627-d284-3b6be2da6073"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "+----+-----+\n",
            "\n",
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|       0|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from wordcount order by count DESC\").show(3)\n",
        "spark.sql(\"select count(*) from wordcount\").show()\n",
        "!cp data-ac.txt streaming\n",
        "# and wait a little\n",
        "!sleep 5s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "louEFkUd8C6y",
        "outputId": "8758de8e-6e44-4f10-a09f-b306ee2b6c48"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "|    |   73|\n",
            "| the|   15|\n",
            "|   a|    9|\n",
            "+----+-----+\n",
            "only showing top 3 rows\n",
            "\n",
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|     611|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from wordcount order by count DESC\").show(3)\n",
        "spark.sql(\"select count(*) from wordcount\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmwJcr3J9V-i",
        "outputId": "16ed5fe5-2822-480d-81df-b7567beb84a5"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "|    |  269|\n",
            "| the|   94|\n",
            "| and|   47|\n",
            "+----+-----+\n",
            "only showing top 3 rows\n",
            "\n",
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|     803|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you understood ??\n",
        "\n",
        "# stopping all active queries\n",
        "query.stop()\n",
        "\n",
        "\n",
        "for q in spark.streams.active:\n",
        "  print(\"stopping\",q.name)\n",
        "  q.stop()\n",
        "\n",
        "# clear the wordcount talbe\n",
        "spark.sql(f\"drop table wordcount\")\n",
        "\n",
        "#clear the streaming directory\n",
        "!rm streaming/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha2ADrGw8pb_",
        "outputId": "7b746038-05f9-43b6-e733-440b5b9cb6cb"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stopping wordcount\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itueIuwNkjLa"
      },
      "source": [
        "As can be seen, Spark counts the number of edits made by each user in the past sixty seconds and emits updates once per second (the original batch duration of the `StreamingContext`)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}